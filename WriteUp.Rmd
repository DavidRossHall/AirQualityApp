---
title: How's the Air Out There? Using a National Air Quality Database to Introduce
  First Year Students to the Fundamentals of Data Analysis
author: "David Hall and Jessica D'eon (Corresponding Author)"
date: "19/03/2021"
output:
  bookdown::html_document2: default
  html_document:
    df_print: paged
  bookdown::word_document2: default
bibliography: references.bib
csl: american-chemical-society.csl
---

```{r setup, include=FALSE}

library(tidyverse)
library(scales)     # for text wrap on x-axis
library(lubridate)  # for app metrics date conversions
library(cowplot)    # for app metrics multiplot

library(ggpubr)
library(ggpmisc)
library(ggExtra)
library(openxlsx)
library(RcppRoll)

library(gridExtra)
library(grid)
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

# Introduction

Whether we like it or not we're living in an age of data, and the world of chemistry is no exception. From big-data atmospheric chemistry in climate-change models[@thefutu2016] to machine-learning organic synthesis[@dealmeida2019], every domain of chemistry is increasingly relying upon data-driven science. Consequently, undergraduate chemistry curriculums need to adapt to better prepare the next-generation of chemists. An oft-overlooked aspect of this is how exactly data (measurements, signals, etc.) are transformed into information (trends, correlation) and finally into knowledge. Moreover, the explicit teaching of these concepts is often neglected resulting in creasing student frustration. Motivated by this, and the need to transfer to a virtual laboratory environment as a result of Covid-19 social distancing restrictions, we sough to develop a new, remove learning compatible, experiment.

Whether we like it or not we're living in an age of data, and the world of chemistry is no exception. From big-data atmospheric chemistry in climate-change models to machine-learning organic synthesis[@dealmeida2019], every domain of chemistry is increasingly relying upon data-driven science. Consequently, undergraduate chemistry curriculums need to adapt to better prepare the next-generation of chemists. An oft-overlooked aspect of this is how exactly data (measurements, signals, etc.) are transformed into information (trends, correlation) and finally into knowledge. Moreover, the explicit teaching of these concepts is often neglected resulting in creasing student frustration. Motivated by this, and the need to transfer to a virtual laboratory environment as a result of Covid-19 social distancing restrictions, we sough to develop a new, remove learning compatible, experiment.


Real data is often imperfect and permeated with outliers and the fingerprints of gross experimental errors; data soon to be collected by undergraduate chemist is no different. However, acquiring sufficient data for analysis if often stressful for undergraduate students given time- and equipment-constraints in the teaching laboratory. To compensate for this, we chose to integrate actual measurements of atmospheric chemicals

Prominent atmospheric pollutants are structurally simple, and undergo reaction schemes comparable to those covered in introductory chemistry lectures. Ozone (O~3~) and nitrogen dioxide (NO~2~) are two choice candidates for analysis by undergraduate students. They are structurally simple molecules, and undergo reaction schemes comparable to those covered in introductory chemistry lectures. Notable of these compounds is their interdependent diurnal cycles. The relationship between O3\~ and NO~2~ is so intimate, the term "odd-oxygen" (O~x~) is used to express the sum of these two compounds, although the between O~3~ and NO~2~ can vary with environmental and anthropogenic influences. Fortunately, with hourly measurements of O~3~ and NO~2~ since 1975, Environment and Climate Change Canada (ECCC) National Airborne Pollutants Surveillance (NAPS) program, there is no shortage of data to be analyzed.

Our Air quality lab described herein, is the result of our efforts. In this new experiment, first year students are introduced to fundamental data analysis concepts as they explore some of the chemistry of atmospheric chemical pollutants.

# Experimental Overview And Pedagogical Goals

This 3 hr data-analysis laboratory exercise uses publicly available data and open-source code (described in the Supplementary information), and has been run successfully in in the one-semester general chemistry course entitled "Chemistry: Physical Principles" at the University of Toronto since Summer 2020. This course is most often conducted in the first-term of the first-year of life-sciences/chemistry students. This exercise is the first-lab of five and is designed as much as an tutorial on data-analysis and Microsoft excel as it is to explore atmospheric chemistry. It is divided into three parts: the prelab, data analysis in Excel, and eventually data exploration and hypothesis generation.

The prelab follows the traditional approach, and is written to situate students in the relevant chemistry for the upcoming analysis. Specifically for this lab we create explanatory videos and material introducing gas phase chemistry, and relating the lab content to concurrent lecture material.

In the data analysis portion of the lab students are randomly assigned two datasets. Each dataset is a 7-day snapshots of hourly O~3~ and NO~2~ measurements taken from the NAPS program. The datasets are all from the same NAPS surveillance station for a given year. For our purposes we chose a different downtown Toronto NAPS station for each successive iteration of the lab. The two datasets correspond to 7-days in the winter and 7-days in the summer. How these datasets are generated are discussed below. Students are provided with a written handbook detailing the necessary Excel operations, and an synchronous online session with their TA. Working through the lab exercises students are explicitly taught data analysis workflows, modelled after that recommended by Hadley and Grolemund[@Wickham2017]:

1.  *Importing* their assigned comma serrated values (.csv) data sets into Excel.
2.  *Tidying* their data and setting up their worksheets. This step consists of formatting cells to properly display values and handling missing data. Specifically for this lab, the NAPS dataset stores missing values as '-999', which can be interpreted literally by Excel.
3.  *Visualizing* their quantitative data by creating a time-series plot of time vs. concentration of each pollutant, see Figure \@ref(fig:example-plots).
4.  *Transforming* their data using mathematical operators in Excel to calculate total oxidant and adding it to their time-series plot as well as calculating 8hr moving averages.
5.  *Modelling* a linear relationship between [O~3~] and [NO~2~] to qualitatively assess the negative relationship between these two contaminants.
6.  *Communicating* and exploring their results through a series of accompanying questions.

```{r example-plots, fig.margin=TRUE, echo = FALSE, message = FALSE, warning=FALSE, fig.height = 7, fig.cap = "Example of plots students are expected to create. (A) time-series of pollutants across 7 winter days. (B) Correlation plot of O3 and NO2 concentrations with linear regression in the winter and (C) summer data sets. (D) Example plot if a '-999' value wasn't removed."}

data <- read.csv("./dataForPaper/Toronto_60410_2018_Day10to16.csv", header = TRUE)

dataSummer <- read.csv("./dataForPaper/Toronto_60410_2018_Day189to195.csv", header = TRUE)

data <- data %>%
  mutate(time = convertToDateTime(data$Date, origin = "1900-01-01")) %>%
  filter(O3 != -999) %>%
  filter(NO2 != -999) %>%
  mutate(OX = NO2 + O3)

### Making data tidyR friendly --------------------------------------------------
dataCol <- data %>%
  select(-c("Date")) %>%
  pivot_longer(-time, names_to = "pollutant", values_to = "concentration")


dataSummer <- dataSummer %>%
  mutate(time = convertToDateTime(dataSummer$Date, origin = "1900-01-01")) %>%
  filter(O3 != -999) %>%
  filter(NO2 != -999) %>%
  mutate(OX = NO2 + O3)

### Making data tidyR friendly --------------------------------------------------
dataSummerCol <- dataSummer %>%
  select(-c("Date")) %>%
  pivot_longer(-time, names_to = "pollutant", values_to = "concentration")

### Time series ----------------
a <- ggplot(data = dataCol, aes(x = time, y = concentration, color = pollutant)) +
  geom_line(size = 1) +
  theme_classic() +
   theme(text = element_text(size = 12),
         legend.position = "right") +
  ylab(bquote('Conc., ppb')) +
  xlab(bquote('Time')) 

### Correlation plot with Linear regression and equation -------------------------

formula <- y ~ x ### Need to keep this so LM regression appears on plot

b <- ggplot(data = data, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_smooth(method = "lm", formula = formula, se = FALSE) +
    stat_poly_eq(aes(label =  paste(stat(rr.label), sep = "*\", \"*")),
               formula = formula, rr.digits = 4 , parse = TRUE, label.y = 0.25, label.x = 0.95, size = 4)+
  annotate("text", x =35, y = 33, label = "winter")

c <- ggplot(data = dataSummer, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_smooth(method = "lm", formula = formula, se = FALSE) +
    stat_poly_eq(aes(label =  paste(stat(rr.label), sep = "*\", \"*")),
               formula = formula, rr.digits = 4 , parse = TRUE, label.y = 0.25, label.x = 0.95, size = 4) +
  annotate("text", x =36, y = 33, label = "summer")

# -999 value for example

dataSummer[2,3] = -999
  
d <- ggplot(data = dataSummer, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(-1050, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_segment(aes(x = 22, y = -600, xend = 16.5, yend = -950),
                  arrow = arrow(length = unit(0.5, "cm"))) +
  annotate("text", x = 32, y = -550, label = "error from analysis")

gt <- arrangeGrob(a, b, c, d,                               
             ncol = 2, nrow = 2)
# Add labels to the arranged plots
p <- as_ggplot(gt) +                                # transform to a ggplot
  draw_plot_label(label = c("A", "B", "C", "D"), size = 16,
                  x = c(0, 0.5, 0, 0.5), y = c(1, 1, 0.5, 0.5)) # Add labels
p

```

The last step in this workflow is expanded in the final part of the lab where students compare their results to the complete NAPS dataset from which their assigned datasets originated from. Here they are encouraged to generate hypotheses based on their own data, and their *a priori* chemical knowledge introduced in the prelab. To this end, we created an interactive online application that students visit using *R* and *Shiny*. This application consist of an interactive map showing the location, and local population, of every NAPS surveillance station. Students can then select any station and time-span, and a time-series and correlation plot, similar to the ones they created themselves, are automatically generated. This allows them to rapidly compare their data to any number of stations, simultaneously relieving them of the burden of repetitive and tedious data analysis and facilitating hypothesis generation and data exploration. Accompanying questions prompt students to explore and reason differences in O~3~ and NO~2~ correlation between urban and rural areas, as well as between winter and summer datasets. See the [Supporting Information] or <https://davidrosshall.shinyapps.io/AirQualityApp/> for details on the application.

## Leveraging R to automate and expand the lab

We made prodigeous use of *R* and various associated frameworks to greatly facilitate many aspects of this lab. Greater details and source code can be found in the [Supporting Information], but a brief discussion is warranted, if anythigne lse, to help encourage readers to harness *R* (or similar data science languages) to both simplify their own workload, while expanding course content.

Firstly, the generation of the 7-day datasets. The NAPS hourly information is recorded for each individual station in separate `.csv` files. These expansive datasets include the measurements from every NAPS stations (n \> 150), and are organized in a matrix style. For first-year students, accessing, subsetting, and tidying these datasets is immediately intimidating and tedious as they often exceed tens of thousands of rows and contain a number of information not necessary for their work. To counteract this, we used *R* to combine O~3~ and NO~2~ measurements, remove bilingual headers, ancillary columns, etc., and transformed the data from the 'wide' matrix style to the 'long' columnar format so data is easier to manipulate in Excel. Then, using *R*, we generate a specific number of student datasets using a 7-day moving window of the year-long data. I.e. data set 1 is Jan. 1st to the 7th, data set 2 is Jan 2nd to 8th, etc, with complimentary summer data sets taken from July 1st onward. The rolling window ensure every student can be assigned a unique dataset, while largely looking at similar data. We also randomly insert a '-999' missing value into each data set ensuring students will encounter it during their analysis. Each data set is then automatically saved as a .csv file.

Secondly, we wrote an *R markdown* script that generates a PDF with the analysis results of every generated data sets. The answer sheet analysis mirrors the one students carry out, providing TAs with an actual analysis of every dataset, allowing them to quickly check each students submission, and relieving them of the burden of verifying each dataset.

Thirdly, using we wrote an interactive application using *R* and *Shiny*. This was created in-house specifically for this laboratory exercise. Thanks to the this, we were able to expand students working data from that directly provided to them, to the entire NAPS dataset. This would be impossible otherwise as students would not have the ability or time to explore the larger NAPS data in any practical manner given the tools we can provide to them. As well, by allowing students to explore the entire dataset, it creates opportunities for them to explore data 'unknown' to the instructors. In other words, students are excited to make real connections and discoveries with real data, rather then tediously analysis an increasing number of provided data. See the [Supplementary Information] for more details on the app, and how you can recycle our code to create your own version of the application if you want to run this lab.

Lastly, our code can accept any standard formatted NAPS dataset, and instructors can readily select the NAPS station, the number of datasets, the overlap between datasets, etc. so course material can easily be updated for each iteration of the lab, or between different lab sections. See the [Supplementary Information] for the source-code for instructions on generating datasets.

# Results and Pedagogical Outcomes

```{r  importing-survey-results, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

surveyData <- read_csv("./dataForPaper/Lab1SurveyData.csv") %>%
  filter(time == "Apr. 2021") %>%
  group_by(QuestionID) %>%
  mutate( percent = count / sum(count)) %>%
  ungroup()

```

Based on the our surveys and feedback from students, the majority of students responded positively to the new learning experience. From our survey of students in the most recent interation of the Air Quality Lab in the winter 2021 term (n=`r sum(subset(surveyData, QuestionID == 4)$count)`), 68% of respondents stated they felt the Excel component of Lab 1 significantly helped them complete the other CHM135 labs more efficiently. (Figure \@ref(fig:plot-survey-results)). This is a welcomed improvement as students frequently complain about the time commitment required for the CHM135 lab component. Students also feel more confident with regards to overall interpretation of data (plots, trendlines) as well as towards their use of Excel in general (57%, and 64%, respectively) (Figure \@ref(fig:plot-survey-results)).

```{r  plot-survey-results, fig.height = 7, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap="End of term student survey results for Lab 1."}

surveyPlot <- function(q, df, axisWidth = 30, subWidth = 40){
  
  df <- subset(df, QuestionID == q) 
  
  question <- str_wrap(unique(df$Question), subWidth)

  ggplot(df, aes(x = Answer, y = percent)) +
    geom_segment( aes(x=Answer, xend=Answer, y=0, yend=percent), colour = "#BB133E") +
    geom_point( color="#00204E", size=4) +
    labs(subtitle = question) +
    ylab("") +
    xlab("") +
    expand_limits(y = 1) +
    coord_flip() +
    scale_y_continuous(minor_breaks = seq(0, 1, 0.1)) +
    scale_y_continuous(labels=percent) +
    scale_x_discrete(labels = label_wrap(40)) +
    theme_light() +
    theme(
      panel.grid.major.y = element_blank(),
      panel.border = element_blank(),
      axis.ticks.y = element_blank(),
      text = element_text(size=10)
      )

}

p1 <- surveyPlot(q = 2, df = surveyData)
p2 <- surveyPlot(q = 3, df = surveyData)
p3 <- surveyPlot(q = 4, df = surveyData)

#gridExtra::grid.arrange(p1, p2, p3, ncol = 1)
cowplot::plot_grid(p1, p2, p3, ncol = 1, align = "v")

```

Included in the survey was the option for students to provide any additional feedback on the lab. STudents expressed both positive and negative feedback to the Air Quality lab (See [Supporting Information] Table \@ref(fig:lab1-Feedback) for complete feedback). Students appreciated the introduction to Excel, the practical usefulness of the incorporated material, and the opportunity to analyze real world data offering a glimpse into environmental chemistry. However, students were also critical of how the material was implemented. Some experienced trouble with inconsistencies between the Excel instructions, and their version of Excel (although all UofT students are provided with free access to the latest version of Excel, with instructions provided in the aforementioned instructions document). Likewise, many students felt the Excel component should have been explicitly tough during the synchronous session, rather than these sessions focusing explicitly on lab material (i.e. data analysis vs. Excel operations). As there are no explicit prerequisites to knowing Excel for this course (or UofT overall), we opted not to directly teach students the basic workings of Excel in the synchronous session. Going forward, we feel that incorporating optional Excel help-sessions specifically to assist students with this component of the lab.

## Notes on the Air Quality Shiny App

A major addition to this lab is the development of the Air Quality App using Shiny. This app is what allows students to readily explore the larger NAPS dataset without burdening them with lengthy data prep/analysis. While the idea of creating an app for a specific lab exercise may seem daunting, the flexibility and support of the Shiny framework greatly relieve one from the minutia of app development, allowing them to focus on how best to present their data to the target audience. The version of our app presented to students in the Winter 2021 term is available for viewing here: <https://davidrosshall.shinyapps.io/AirQualityApp/>. We have also provided the complete source code and example datasets on Github: <https://github.com/DavidRossHall/AirQualityApp>, and as a `.zip` in the supplementary information. Alongside these are instructions modifying, and running, this app for your course.

We chose not to capitalize on the abilities of Google analytics (although that is an option when creating Shiny Apps) to track student usage of the app. Instead, we were inferred app usage by tracking the number of accessions (i.e. times the app was used). As shown in Figure \@ref(fig:app-connections), despite publishing the course material more than a week prior to the synchronous lab sessions, students did not access the app in any meaningful numbers. Usage did increase after the synchronous session, where students were explicitly instructed to work on their Lab 1 reports, of which two questions specifically instruct students to access the Air Quality App. Predictably, app usage was highest immediately preceding the due date for the Lab 1 report; the implications of this are discussed below. As best we can tell, usage/interaction with the Air Quality app lasted on average 25 minutes, a respectable time given the brevity of the prompting questions, and the richness of the dataset.

```{r app-connections_data-manipulation, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

# Container data spanning lab period

df <- read.csv("./dataForPaper/container_status_Practical1.csv") %>% 
  mutate(date=as.POSIXct(as.numeric(as.character(timestamp)),origin="1970-01-01")) %>% 
  mutate(connect_count = as.numeric(connect_count)) %>%
  mutate(connect_procs = as.numeric(connect_procs))

# Subsetting data to cover Exp 1 period & getting number of connections

df1 <- df %>%
  select(-timestamp) %>% 
  filter(date > ymd_hms("2021-01-10 00:00:00") & date < ymd_hms("2021-01-31 00:00:00")) %>%
  arrange(date) %>% 
  mutate(
    n_count=cumsum(connect_count),
    n_procs=cumsum(connect_procs),
    new_connect=case_when(
      connect_count>lag(connect_count,1) ~ connect_count-lag(connect_count,1),
      TRUE ~ 0),
    n_connect=cumsum(new_connect) # approximate
  ) %>% 
  filter(n_count>0)

# Prep for cumsum plot

df2 <- df1 %>%  
  select(n_connect, date) %>% 
  gather(key="key", value="value", -date)

# getting counts per day for bar plot.

df3 <- df1 %>%
  mutate(date2 = as.Date(date)) %>%
  group_by(date2) %>%
  summarize(size = max(n_connect)) %>%
  mutate(dayCount = size - lag(size))

```

```{r app-connections, fig.height = 2.5, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap="Connections to Air Quality app per day (top) and cummulative connections over time (bottom). Vertical lines indicate when the lab material was made available online to students, when the synchronous sessions with TAs started, and the due date. Blue rectangle highlights period when students worked on report sheet."}

p1 <- ggplot(df3, aes(x = date2, y = dayCount)) +
  geom_bar(fill="gray", stat = "identity") +  theme(axis.title=element_blank()) +
  scale_y_continuous(breaks=seq(0, 70, 20)) +
  labs(y = "Connections\nper day") +
  theme_classic() +
  theme(axis.line.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank())
  

p2 <- ggplot(df2) +
  labs(x="Date", y="Cumulative\nConnections") +
  geom_line(aes(x=date, y=value)) + 
  geom_vline(xintercept = ymd_hms("2021-01-10 18:00:00"), color = "black", lwd = 1) + #lab published on quercus
  geom_vline(xintercept = ymd_hms("2021-01-21 09:00:00"), color = "black", lwd = 1) + #Sync sessions starts
  geom_vline(xintercept = ymd_hms("2021-01-27 22:00:00"), color = "black", lwd = 1) + #due date 
  annotate("rect", xmin = ymd_hms("2021-01-21 09:00:00"), xmax = ymd_hms("2021-01-27 22:00:00"), 
           ymin = 0, ymax = max(df2$value), alpha = .1,fill = "blue") + # rect for when students actively working
  annotate("text", x = ymd_hms("2021-01-11 00:00:00"), y = 250, label = "Released", hjust = 0, size=3) +
  annotate("text", x = ymd_hms("2021-01-21 00:00:00"), y = 125, label = "Start sync. \nsessions", hjust = 1, size=3) +
  annotate("text", x = ymd_hms("2021-01-27 23:59:00"), y = 125, label = "Due date", hjust = 0, size=3) +
  scale_x_datetime(date_breaks = "7 day", date_labels = "%b %d") +
  theme_classic()




#grid.arrange(p2, p, ncol = 1)
plot_grid(p1, p2, ncol = 1, align = "v")

```

A brief comment for the unaware: running a Shiny app requires server side computing. In other words, as students select data to plot, a dedicated server must perform the necessary computations. While these requirements are not egregious, they are still to be considered if you plan on hosting your own instance of the app. While Shiny provides instructions and software for running the app on premise, we opted to host our app on the Shiny server cloud.As mentioned above, many students 'flashed' the app as the deadline appeared. Without adequate server space, this could increase load times, decrease responsiveness, and possibly crash the app. Shiny offers 15hrs/month of server time (time it takes to run the app), but this is unsuitable for the anticipated server loads once the App was released for all of the CHM135 lab sections. Consequently, for the 2021 Winter Term, there were slightly over ??? students, so we payed the *Standard* hosting package costing \$99USD/month for 2000 hrs of server time, in addition to using the reliable Shiny servers. This was admittedly an excess of server time, but we chose to play it safe. Likewise, as the Air Quality App was only used during the first lab, we only needed to pay for one month of server time to account for all the lab sections. As we continue to experiment with creating purpose built apps for individual labs/courses, we will transition to local hosting.

# Conclusions

We sought to create a new introductory lab experiment to expressly teach incoming students foundational components of data analysis/science as well as practical instructions on how to use Microsoft Excel for future courses. To this end, we leveraged the R computer language for the automated, and scalable, generation of unique data from real world atmospheric measurements from the NAPS that served as the foundation of our introductory data science lab. Alongside written instructions on how to use Excel, we developed an online interactive App, allowing students to readily explore the entire NAPS dataset to compliment their individual analyses. Our efforts were rewarded with students being better equipped to tackle subsequent data analysis challenges in the following lab, in addition to arming them with skills they will assuredly make use of outside of the first-year chemistry laboratory.

# Supporting Information

*Note this will be a seperate document, obviously*.

Stuff to include:

-   Images of app
-   

```{r lab1-Feedback, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

surveyQ1 <- surveyData %>%
  filter(QuestionID == 1)

Q1 <- paste( "Student responses to: ", unique(surveyQ1$Question))

knitr::kable(select(surveyQ1, Answer), caption = (Q1))

```

```{r chm135-Feedback, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

surveyQ5 <- surveyData %>%
  filter(QuestionID == 5)

Q5 <- paste( "Student responses to: ", unique(surveyQ5$Question))

knitr::kable(select(surveyQ5, Answer), caption = (Q5))

```

# Author Information

David Hall, Department of Chemistry and School of the Environment, University of Toronto.

Jessica D'eon, Department of Chemistry and School of the Environment, University of Toronto.

# Acknowledgements

# References
