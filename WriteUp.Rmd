---
title: How's the Air Out There? Using a National Air Quality Database to Introduce
  First Year Students to the Fundamentals of Data Analysis
author: "David Hall and Jessica D'eon (Corresponding Author)"
date: "19/03/2021"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}

library(tidyverse)
library(scales)     # for text wrap on x-axis
library(lubridate)  # for app metrics date conversions
library(cowplot)    # for app metrics multiplot

knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

# Introduction

Whether we like it or not we're living in an age of data, and the world of chemistry is no exception. From big-data atmospheric chemistry in climate-change models to machine-learning organic synthesis, every domain of chemistry is increasingly relying upon data-driven science. Consequently, undergraduate chemistry curriculums need to adapt to better prepare the next-generation of chemists. An oft-overlooked aspect of this is how exactly data (measurements, signals, etc.) are transformed into information (trends, correlation) and finally into knowledge. Moreover, the explicit teaching of these concepts is often neglected resulting in creasing student frustration. Motivated by this, and the need to transfer to a virtual laboratory environment as a result of Covid-19 social distancing restrictions, we sough to develop a new, remove learning compatible, experiment. 

Real data is often imperfect and permeated with outliers and the fingerprints of gross experimental errors; data soon to be collected by undergraduate chemist is no different. However, acquiring sufficient data for analysis if often stressful for undergraduate students given time- and equipment-constraints in the teaching laboratory. To compensate for this, we chose to integrate actual measurements of atmospheric chemicals

Prominent atmospheric pollutants are structurally simple, and undergo reaction schemes comparable to those covered in introductory chemistry lectures. Ozone (O~3~) and nitrogen dioxide (NO~2~) are two choice candidates for analysis by undergraduate students. They are structurally simple molecules, and undergo reaction schemes comparable to those covered in introductory chemistry lectures. Notable of these compounds is their interdependent diurnal cycles. The relationship between O3~ and NO~2~ is so intimate, the term "odd-oxygen" (O~x~) is used to express the sum of these two compounds, although the between O~3~ and NO~2~  can vary with environmental and anthorpogenic influences. Fortunately, with hourly measurements of O~3~ and NO~2~ since 1975, Environment and Climate Change Canada (ECCC) National Airborne Pollutants Surveillance (NAPS) program, there is no shortage of data to be analyzed.

Our Air quality lab described herein, is the result of our efforts. In this new experiment, first year students are introduced to fundamental data analysis concepts as they explore some of the chemistry of atmospheric chemical pollutants.  

# Experimental Overview And Pedagogical Goals

This 3 hr data-analysis laboratory exercise uses publicly available data and open-source code (described in the Supplementary information), and has been run successfully in in the one-semester general chemistry course entitled "Chemistry: Phyiscal Principles" at the University of Toronto since Summer 2020. This course is most often conducted in the first-term of the first-year of life-sciences/chemistry students. This exercise is the first-lab of five and is designed as much as an tutorial on data-analysis and Microsoft excel as it is to explore atmospheric chemistry. It is divided into three parts: 

Part A consist of pre-laboratory exercises featuring explanatory videos and material introducing gas phase chemistry questions. The aim of this part is to situate students in the chemistry that forms the basis of their data analysis. 

In Part B students are randomly assigned a 7-day snapshot of hourly O~3~ and NO~2~ measurements  


# Results and Pedagogical Outcomes



```{r  importing-survey-results, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

surveyData <- read_csv("./dataForPaper/Lab1SurveyData.csv") %>%
  filter(time == "Apr. 2021") %>%
  group_by(QuestionID) %>%
  mutate( percent = count / sum(count)) %>%
  ungroup()

```

From n=`r sum(subset(surveyData, QuestionID == 4)$count)`


```{r  plot-survey-results, fig.height = 7, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

surveyPlot <- function(q, df, axisWidth = 30, subWidth = 40){
  
  df <- subset(df, QuestionID == q) 
  
  question <- str_wrap(unique(df$Question), subWidth)

  ggplot(df, aes(x = Answer, y = percent)) +
    geom_segment( aes(x=Answer, xend=Answer, y=0, yend=percent), colour = "#BB133E") +
    geom_point( color="#00204E", size=4) +
    labs(subtitle = question) +
    ylab("") +
    xlab("") +
    expand_limits(y = 1) +
    coord_flip() +
    scale_y_continuous(minor_breaks = seq(0, 1, 0.1)) +
    scale_y_continuous(labels=percent) +
    scale_x_discrete(labels = label_wrap(40)) +
    theme_light() +
    theme(
      panel.grid.major.y = element_blank(),
      panel.border = element_blank(),
      axis.ticks.y = element_blank(),
      text = element_text(size=10)
      )

}

p1 <- surveyPlot(q = 2, df = surveyData)
p2 <- surveyPlot(q = 3, df = surveyData)
p3 <- surveyPlot(q = 4, df = surveyData)

#gridExtra::grid.arrange(p1, p2, p3, ncol = 1)
cowplot::plot_grid(p1, p2, p3, ncol = 1, align = "v")

```


```{r app-connections_data-manipulation, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

# Container data spanning lab period

df <- read.csv("./dataForPaper/container_status_Practical1.csv") %>% 
  mutate(date=as.POSIXct(as.numeric(as.character(timestamp)),origin="1970-01-01")) %>% 
  mutate(connect_count = as.numeric(connect_count)) %>%
  mutate(connect_procs = as.numeric(connect_procs))

# Subsetting data to cover Exp 1 period & getting number of connections

df1 <- df %>%
  select(-timestamp) %>% 
  filter(date > ymd_hms("2021-01-10 00:00:00") & date < ymd_hms("2021-01-31 00:00:00")) %>%
  arrange(date) %>% 
  mutate(
    n_count=cumsum(connect_count),
    n_procs=cumsum(connect_procs),
    new_connect=case_when(
      connect_count>lag(connect_count,1) ~ connect_count-lag(connect_count,1),
      TRUE ~ 0),
    n_connect=cumsum(new_connect) # approximate
  ) %>% 
  filter(n_count>0)

# Prep for cumsum plot

df2 <- df1 %>%  
  select(n_connect, date) %>% 
  gather(key="key", value="value", -date)

# getting counts per day for bar plot.

df3 <- df1 %>%
  mutate(date2 = as.Date(date)) %>%
  group_by(date2) %>%
  summarize(size = max(n_connect)) %>%
  mutate(dayCount = size - lag(size))

```

```{r app-connections, fig.height = 2.5, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap="Connections to Air Quality app per day (top) and cummulative connections over time (bottom). Vertical lines indicate when the lab material was made available online to students, when the synchronous sessions with TAs started, and the due date. Blue rectangle highlights period when students worked on report sheet."}

p1 <- ggplot(df3, aes(x = date2, y = dayCount)) +
  geom_bar(fill="gray", stat = "identity") +  theme(axis.title=element_blank()) +
  scale_y_continuous(breaks=seq(0, 70, 20)) +
  labs(y = "Connections\nper day") +
  theme_classic() +
  theme(axis.line.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank())
  

p2 <- ggplot(df2) +
  labs(x="Date", y="Cumulative\nConnections") +
  geom_line(aes(x=date, y=value)) + 
  geom_vline(xintercept = ymd_hms("2021-01-10 18:00:00"), color = "black", lwd = 1) + #lab published on quercus
  geom_vline(xintercept = ymd_hms("2021-01-21 09:00:00"), color = "black", lwd = 1) + #Sync sessions starts
  geom_vline(xintercept = ymd_hms("2021-01-27 22:00:00"), color = "black", lwd = 1) + #due date 
  annotate("rect", xmin = ymd_hms("2021-01-21 09:00:00"), xmax = ymd_hms("2021-01-27 22:00:00"), 
           ymin = 0, ymax = max(df2$value), alpha = .1,fill = "blue") + # rect for when students actively working
  annotate("text", x = ymd_hms("2021-01-11 00:00:00"), y = 250, label = "Released", hjust = 0, size=3) +
  annotate("text", x = ymd_hms("2021-01-21 00:00:00"), y = 125, label = "Start sync. \nsessions", hjust = 1, size=3) +
  annotate("text", x = ymd_hms("2021-01-27 23:59:00"), y = 125, label = "Due date", hjust = 0, size=3) +
  scale_x_datetime(date_breaks = "7 day", date_labels = "%b %d") +
  theme_classic()




#grid.arrange(p2, p, ncol = 1)
plot_grid(p1, p2, ncol = 1, align = "v")

```

# Conclusions

# Supporting Information

# Author Information

# Acknowledgements

# References
